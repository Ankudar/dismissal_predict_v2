from datetime import datetime
import logging

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from rusgenderdetection import get_gender  # type: ignore
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, RobustScaler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DATA_INTERIM = "/home/root6/python/dismissal_predict_v2/data/interim"
DATA_PROCESSED = "/home/root6/python/dismissal_predict_v2/data/processed"
INPUT_FILE_MAIN_USERS = f"{DATA_INTERIM}/main_users.csv"
INPUT_FILE_CADR = f"{DATA_INTERIM}/check_last_users_update.csv"
INPUT_HISTORY_CADR = f"{DATA_INTERIM}/history_cadr_base.csv"
INPUT_FILE_CHILDREN = f"{DATA_INTERIM}/children.csv"
INPUT_FILE_DIRECTOR = f"{DATA_INTERIM}/director.csv"
INPUT_FILE_STAT = f"{DATA_INTERIM}/whisper_stat.csv"
INPUT_ZUP_PATH = f"{DATA_INTERIM}/zup.csv"

DROP_COLS = [
    "id",
    "–ª–æ–≥–∏–Ω",
    "–∏–º—è",
    "—Ñ–∞–º–∏–ª–∏—è",
    "—Ñ–∏–æ",
    "–¥–æ–ª–∂–Ω–æ—Å—Ç—å",
    "–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è",
    "–¥–∞—Ç–∞_—Ä–æ–∂–¥–µ–Ω–∏—è",
    "–¥–∞—Ç–∞_–ø—Ä–∏–µ–º–∞_–≤_1—Å",
    "—Ç–µ–∫—É—â–∞—è_–¥–æ–ª–∂–Ω–æ—Å—Ç—å_–Ω–∞_–ø–æ—Ä—Ç–∞–ª–µ",
    "–æ—Ç–¥–µ–ª",
    "—É—Ä–æ–≤–µ–Ω—å_–∑–ø",
    "–≥—Ä–µ–π–¥",
    "id_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è",
]

FLOAT_COLS = ["—Ç–æ–Ω", "—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ", "–æ—Ñ—Ñ–µ—Ä", "–≤—Ä–µ–¥–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–ª–∏—á–Ω–∞—è –∂–∏–∑–Ω—å", "—Å—Ç—Ä–µ—Å—Å", "–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã"]

LOGINS_TO_REMOVE = [
    "root24",
    "root35",
    "root36",
    "test.testovich",
    "admin",
    "ib_tech",
    "ib_tech2",
    "lab.s10",
    "lab.s12",
    "lab.s13",
    "lab.s3",
    "lab.t20",
    "test.ap",
    "test_top",
    "test7",
    "testacc",
    "testacc1",
    "testacc12",
    "testacc14",
    "testacc16",
    "testacc17",
    "testacc18",
    "testacc19",
    "testacc2",
    "testacc20",
    "testacc21",
    "testacc22",
    "testacc3",
    "testacc4",
    "testacc5",
    "testacc7",
    "testacc9",
    "testacc10",
    "testacc11",
    "teacher.eng.11",
    "wf_jurist",
    "wf_sale",
    "wf_sale2",
    "—Å–∏—Å—Ç–µ–º–∞",
]

main_users = pd.read_csv(INPUT_FILE_MAIN_USERS, delimiter=",", decimal=",")
users_cadr = pd.read_csv(INPUT_FILE_CADR, delimiter=",", decimal=",")
history_cadr = pd.read_csv(INPUT_HISTORY_CADR, delimiter=",", decimal=",")
users_salary = pd.read_csv(INPUT_ZUP_PATH, delimiter=",", decimal=",")
children = pd.read_csv(INPUT_FILE_CHILDREN, delimiter=",", decimal=",")
director = pd.read_csv(INPUT_FILE_DIRECTOR, delimiter=",", decimal=",")
stat = pd.read_csv(INPUT_FILE_STAT, delimiter=",", decimal=",")


director = director[["id", "id_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è"]].copy()


class DataPreprocessor:
    def __init__(self):
        self.cat_cols = []
        self.ordinal_encoder = None
        self.onehot_encoder = None
        self.scaler = None
        self.numeric_cols = []
        self.preprocessor = None

    def save(self, path: str):
        joblib.dump(self, path)

    @staticmethod
    def load(path: str):
        return joblib.load(path)

    def drop_trash_feature(self, df, threshold=0.9):
        high_nan_cols = df.columns[df.isnull().mean() > threshold].tolist()
        if high_nan_cols:
            print(f"–£–¥–∞–ª–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º % NaN: {high_nan_cols}")
            df = df.drop(columns=high_nan_cols)
        return df

    # def drop_trash_rows(self, df, threshold=0.5):
    #     row_nan_fraction = df.isnull().mean(axis=1)
    #     bad_rows = df.index[row_nan_fraction > threshold]
    #     if len(bad_rows) > 0:
    #         print(
    #             f"–£–¥–∞–ª–µ–Ω—ã —Å—Ç—Ä–æ–∫–∏ —Å –±–æ–ª–µ–µ —á–µ–º {int(threshold * 100)}% –ø—Ä–æ–ø—É—Å–∫–æ–≤: {len(bad_rows)} —à—Ç."
    #         )
    #         df = df.drop(index=bad_rows)
    #     return df

    def _handle_nans(self, df):
        for col in self.numeric_cols:
            df[col] = pd.to_numeric(df[col], errors="coerce")
            df[col] = df[col].fillna(df[col].median())
        return df

    def fit(self, df: pd.DataFrame):
        df = df.copy()

        df.drop(columns=[col for col in DROP_COLS if col in df.columns], inplace=True)

        uvolen_series = df["—É–≤–æ–ª–µ–Ω"] if "—É–≤–æ–ª–µ–Ω" in df.columns else None

        df = self.drop_trash_feature(df)
        # df = self.drop_trash_rows(df)

        if uvolen_series is not None:
            uvolen_series = uvolen_series.loc[df.index]

        # –£–¥–∞–ª—è–µ–º "—É–≤–æ–ª–µ–Ω" –ü–ï–†–ï–î –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤!
        if "—É–≤–æ–ª–µ–Ω" in df.columns:
            df = df.drop(columns=["—É–≤–æ–ª–µ–Ω"])

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
        self.cat_cols = df.select_dtypes(include=["object"]).columns.tolist()
        # for col in self.cat_cols:
        #     df[col] = df[col].astype(str).fillna("other")

        # –ß–∏—Å–ª–æ–≤—ã–µ
        self.numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()

        df = self._handle_nans(df)

        onehot_cols = [col for col in self.cat_cols if df[col].nunique() <= 15]
        ordinal_cols = [col for col in self.cat_cols if df[col].nunique() >= 16]

        self.onehot_encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
        self.ordinal_encoder = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)

        self.preprocessor = ColumnTransformer(
            transformers=[
                ("onehot", self.onehot_encoder, onehot_cols),
                ("ordinal", self.ordinal_encoder, ordinal_cols),
                ("num", RobustScaler(), self.numeric_cols),
            ],
            remainder="drop",
        )

        df_transformed = self.preprocessor.fit_transform(df)
        df_transformed = pd.DataFrame(
            df_transformed, columns=self.preprocessor.get_feature_names_out()  # type: ignore
        )

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º '—É–≤–æ–ª–µ–Ω'
        if uvolen_series is not None:
            df_transformed["—É–≤–æ–ª–µ–Ω"] = uvolen_series.values

        return df_transformed

    def transform(self, df: pd.DataFrame):
        df = df.copy()

        # –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
        df.drop(columns=[col for col in DROP_COLS if col in df.columns], inplace=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        uvolen_series = df["—É–≤–æ–ª–µ–Ω"] if "—É–≤–æ–ª–µ–Ω" in df.columns else None
        if uvolen_series is not None:
            uvolen_series = uvolen_series.loc[df.index]

        # –£–¥–∞–ª—è–µ–º "—É–≤–æ–ª–µ–Ω" –ø–µ—Ä–µ–¥ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–æ–º
        if "—É–≤–æ–ª–µ–Ω" in df.columns:
            df = df.drop(columns=["—É–≤–æ–ª–µ–Ω"])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
        # for col in self.cat_cols:
        #     if col not in df.columns:
        #         df[col] = "other"
        #     df[col] = df[col].astype(str).fillna("other")

        df = self._handle_nans(df)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        df_transformed = self.preprocessor.transform(df)  # type: ignore
        df_transformed = pd.DataFrame(
            df_transformed, columns=self.preprocessor.get_feature_names_out()  # type: ignore
        )

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º "—É–≤–æ–ª–µ–Ω"
        if uvolen_series is not None:
            df_transformed["—É–≤–æ–ª–µ–Ω"] = uvolen_series.values

        return df_transformed


def merge_base(bases, index, merge_type):
    try:
        merged_df = bases[0]
        for base in bases[1:]:
            merged_df = pd.merge(merged_df, base, on=index, how=merge_type)
        return merged_df
    except Exception as e:
        logger.info(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏: {e}")
        raise


def merge_fillna(left_df, right_df, on="—Ñ–∏–æ"):
    # –°–ª–∏–≤–∞–µ–º –ø–æ –∫–ª—é—á—É
    merged = pd.merge(left_df, right_df, on=on, how="outer", suffixes=("_left", "_right"))

    result = merged[[on]].copy()

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏, –∫—Ä–æ–º–µ –∫–ª—é—á–∞
    for col in left_df.columns:
        if col == on:
            continue

        col_left = f"{col}_left"
        col_right = f"{col}_right"

        if col_left in merged.columns and col_right in merged.columns:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –æ–±–∞ ‚Äî –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏–∑ –ø—Ä–∞–≤–æ–≥–æ
            result[col] = merged[col_left].combine_first(merged[col_right])
        elif col_left in merged.columns:
            result[col] = merged[col_left]
        elif col_right in merged.columns:
            result[col] = merged[col_right]

    return result


def convert_dates(df):
    try:
        date_columns = ["–¥–∞—Ç–∞_—Ä–æ–∂–¥–µ–Ω–∏—è", "–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è", "–¥–∞—Ç–∞_–ø—Ä–∏–µ–º–∞_–≤_1—Å"]

        # –ó–∞–º–µ–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–∞ '1970-01-01 00:00:00'
        for col in date_columns:
            df[col] = df[col].fillna("1970-01-01 00:00:00")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ datetime —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        df["–¥–∞—Ç–∞_—Ä–æ–∂–¥–µ–Ω–∏—è"] = pd.to_datetime(
            df["–¥–∞—Ç–∞_—Ä–æ–∂–¥–µ–Ω–∏—è"], format="%d.%m.%Y", errors="coerce"
        )
        df["–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"] = pd.to_datetime(
            df["–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"], format="%d.%m.%Y %H:%M:%S", errors="coerce"
        )
        df["–¥–∞—Ç–∞_–ø—Ä–∏–µ–º–∞_–≤_1—Å"] = pd.to_datetime(
            df["–¥–∞—Ç–∞_–ø—Ä–∏–µ–º–∞_–≤_1—Å"], format="%Y-%m-%d", errors="coerce"
        )

        return df
    except Exception as e:
        logger.info(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –¥–∞—Ç: {e}")
        raise


def mode_with_tie(series):
    try:
        if series.empty:
            return "none"

        counts = series.value_counts()
        m_count = counts.get("m", 0)
        f_count = counts.get("f", 0)

        if m_count > f_count:
            return "m"
        elif f_count > m_count:
            return "f"
        else:
            return "mf"
    except Exception as e:
        logger.info(f"–û—à–∏–±–∫–∞ –≤ mode_with_tie: {e}")
        raise


def extract_first_name(full_name):
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
    if pd.isna(full_name):
        return ""
    parts = full_name.split()
    return parts[1] if len(parts) > 1 else ""


def determine_gender(full_name):
    first_name = extract_first_name(full_name)
    if first_name:
        gender = get_gender(first_name)
        if gender == 0:
            return "–∂–µ–Ω—Å–∫–∏–π"
        elif gender == 1:
            return "–º—É–∂—Å–∫–æ–π"
        else:
            return "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    return "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"


def main_prepare_for_all(main_users, users_salary, users_cadr, children):
    try:
        today = pd.to_datetime(datetime.now().date())

        # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–µ—Ç–µ–π ---
        children["date_birth"] = pd.to_datetime(
            children["date_birth"], format="%d.%m.%Y", errors="coerce"
        )
        children["age"] = ((datetime.now() - children["date_birth"]).dt.days / 365).round(1)

        grouped_children = (
            children.groupby("id")
            .agg(
                —á–∏—Å–ª–æ_–¥–µ—Ç–µ–π=("name", "count"),
                —Å—Ä–µ–¥–Ω–∏–π_–≤–æ–∑—Ä–∞—Å—Ç_–¥–µ—Ç–µ–π=("age", "mean"),
                —Å—Ä–µ–¥–Ω–∏–π_–ø–æ–ª_–¥–µ—Ç–µ–π=("gender", mode_with_tie),
            )
            .reset_index()
        )

        # --- –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –§–ò–û ---
        for df in [users_salary, users_cadr]:
            df["—Ñ–∏–æ"] = df["—Ñ–∏–æ"].str.split().str[:2].str.join(" ")
        main_users["—Ñ–∏–æ"] = main_users["—Ñ–∞–º–∏–ª–∏—è"] + " " + main_users["–∏–º—è"]

        # --- –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∏—Å—Ç–æ—Ä–∏–µ–π ---
        users_cadr = merge_fillna(users_cadr, history_cadr, on="—Ñ–∏–æ")
        users_cadr.to_csv(
            INPUT_HISTORY_CADR, index=False, sep=",", decimal=",", encoding="utf-8-sig"
        )

        main_users = merge_base([main_users, users_cadr], "—Ñ–∏–æ", "right")
        main_users = merge_base([main_users, users_salary], "—Ñ–∏–æ", "left")
        main_users = merge_base([main_users, grouped_children], "id", "left")
        main_users = merge_base([main_users, director], "id", "left")

        # --- –û—á–∏—Å—Ç–∫–∞ ---
        main_users = main_users[~main_users["–ª–æ–≥–∏–Ω"].isin(LOGINS_TO_REMOVE)]
        main_users["–ø–æ–ª"] = main_users["—Ñ–∏–æ"].apply(determine_gender)
        main_users.replace("nan", pd.NA, inplace=True)
        main_users = main_users.dropna(subset=["–∏–º—è", "—Ñ–∞–º–∏–ª–∏—è"])
        main_users = convert_dates(main_users)

        for col in ["–ª–æ–≥–∏–Ω", "–¥–æ–ª–∂–Ω–æ—Å—Ç—å", "–∏–º—è", "—Ñ–∞–º–∏–ª–∏—è"]:
            main_users[col] = main_users[col].astype(str)
        main_users["—Å—Ä_–∑–ø"] = main_users["—Å—Ä_–∑–ø"].astype(float)
        main_users["—É–≤–æ–ª–µ–Ω"] = main_users["–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"].notna().astype(int)

        main_users["–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"] = pd.to_datetime(
            main_users["–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"], errors="coerce"
        )

        # --- –í–æ–∑—Ä–∞—Å—Ç –∏ —Å—Ç–∞–∂ ---
        main_users["–≤–æ–∑—Ä–∞—Å—Ç"] = np.where(
            main_users["–¥–∞—Ç–∞_—Ä–æ–∂–¥–µ–Ω–∏—è"].notna(),
            (today - main_users["–¥–∞—Ç–∞_—Ä–æ–∂–¥–µ–Ω–∏—è"]).dt.days // 365,  # type: ignore
            np.nan,
        )

        main_users["—Å—Ç–∞–∂"] = np.where(
            main_users["–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"].notna(),
            (main_users["–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"] - main_users["–¥–∞—Ç–∞_–ø—Ä–∏–µ–º–∞_–≤_1—Å"]).dt.days / 365,
            (today - main_users["–¥–∞—Ç–∞_–ø—Ä–∏–µ–º–∞_–≤_1—Å"]).dt.days / 365,  # type: ignore
        )
        main_users["—Å—Ç–∞–∂"] = np.maximum(main_users["—Å—Ç–∞–∂"], 0)

        # --- –ö–æ–ª-–≤–æ –ø–æ–¥—á–∏–Ω—ë–Ω–Ω—ã—Ö ---
        sub_count = main_users["id_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è"].value_counts()
        main_users["–ø–æ–¥—á–∏–Ω–µ–Ω–Ω—ã–µ"] = main_users["id"].apply(lambda x: sub_count.get(x, 0))

        # --- –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è ---
        main_users["id_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è"] = main_users["id_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è"].fillna(-1).astype(int)
        main_users["—Å—Ä–µ–¥–Ω–∏–π_–≤–æ–∑—Ä–∞—Å—Ç_–¥–µ—Ç–µ–π"] = (
            main_users["—Å—Ä–µ–¥–Ω–∏–π_–≤–æ–∑—Ä–∞—Å—Ç_–¥–µ—Ç–µ–π"].fillna(0).astype(float)
        )
        main_users["—Å—Ä_–∑–ø"] = main_users["—Å—Ä_–∑–ø"].fillna(0).astype(float)

        # --- –ù–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ---
        main_users["—Å–∫–æ—Ä–æ_–¥—Ä"] = (
            (main_users["–¥–∞—Ç–∞_—Ä–æ–∂–¥–µ–Ω–∏—è"].dt.month == today.month)
            & (abs(main_users["–¥–∞—Ç–∞_—Ä–æ–∂–¥–µ–Ω–∏—è"].dt.day - today.day) <= 30)
        ).astype(int)

        main_users["—Å–∫–æ—Ä–æ_–≥–æ–¥–æ–≤—â–∏–∫–∞_–ø—Ä–∏–µ–º–∞"] = (
            (main_users["–¥–∞—Ç–∞_–ø—Ä–∏–µ–º–∞_–≤_1—Å"].dt.month == today.month)
            & (abs(main_users["–¥–∞—Ç–∞_–ø—Ä–∏–µ–º–∞_–≤_1—Å"].dt.day - today.day) <= 30)
        ).astype(int)

        main_users["–µ—Å—Ç—å_–º–∞–ª–µ–Ω—å–∫–∏–µ_–¥–µ—Ç–∏"] = (
            main_users["id"].map(children.groupby("id")["age"].min()).le(5)
            & (main_users["—á–∏—Å–ª–æ_–¥–µ—Ç–µ–π"] > 0)
        ).astype(int)

        main_users["–∏–Ω–¥–µ—Å_–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏_–∑–∞_–¥–µ—Ç—å–º–∏"] = main_users["—á–∏—Å–ª–æ_–¥–µ—Ç–µ–π"].fillna(0) * (
            18 - main_users["—Å—Ä–µ–¥–Ω–∏–π_–≤–æ–∑—Ä–∞—Å—Ç_–¥–µ—Ç–µ–π"].fillna(0)
        )

        mean_salary = main_users["—Å—Ä_–∑–ø"].replace(0, np.nan).mean()
        main_users["–∑–ø_–Ω–∞_—Å—Ä_–∑–ø_–ø–æ_–∫–æ–º–ø–∞–Ω–∏–∏"] = main_users["—Å—Ä_–∑–ø"] / mean_salary
        main_users["–Ω–µ_–¥–æ–ø–ª–∞—á–∏–≤–∞—é—Ç"] = (main_users["–∑–ø_–Ω–∞_—Å—Ä_–∑–ø_–ø–æ_–∫–æ–º–ø–∞–Ω–∏–∏"] < 0.8).astype(int)

        main_users["—Å—Ç–∞–∂_–Ω–∞_–≤–æ–∑—Ä–∞—Å—Ç"] = main_users["—Å—Ç–∞–∂"] / main_users["–≤–æ–∑—Ä–∞—Å—Ç"]
        main_users["–∑–ø_–Ω–∞_—á–∏—Å–ª–æ_–¥–µ—Ç–µ–π"] = main_users["—Å—Ä_–∑–ø"] / main_users["—á–∏—Å–ª–æ_–¥–µ—Ç–µ–π"].replace(
            0, np.nan
        )
        main_users["–Ω–µ–¥–æ–ø–ª–∞—Ç–∞_–∏_–±–æ–ª—å—à–µ_2_–¥–µ—Ç–µ–π"] = (
            (main_users["–Ω–µ_–¥–æ–ø–ª–∞—á–∏–≤–∞—é—Ç"] == 1) & (main_users["—á–∏—Å–ª–æ_–¥–µ—Ç–µ–π"] >= 2)
        ).astype(int)

        # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
        main_users.to_csv(f"{DATA_PROCESSED}/main_all.csv", index=False)

        preprocessor = DataPreprocessor()
        main_users_for_train = preprocessor.fit(main_users)

        main_users_for_train.to_csv(f"{DATA_PROCESSED}/main_users_for_train.csv", index=False)
        preprocessor.save(f"{DATA_PROCESSED}/preprocessor")

        print(
            "NaNs in main_users_for_train:\n",
            main_users_for_train.isnull().sum()[main_users_for_train.isnull().any()],
        )

    except Exception as e:
        logger.info(f"–û—à–∏–±–∫–∞: {e}")
        raise


def prepare_with_mic():
    main_all = pd.read_csv(f"{DATA_PROCESSED}/main_all.csv", delimiter=",", decimal=",")
    main_top = merge_base([stat, main_all], "–ª–æ–≥–∏–Ω", "right")
    main_top = main_top[main_top["–ª–æ–≥–∏–Ω"].isin(stat["–ª–æ–≥–∏–Ω"])]
    main_top = main_top[~main_top["–ª–æ–≥–∏–Ω"].isin(LOGINS_TO_REMOVE)]

    main_top["id_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è"] = main_top["id_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è"].fillna(-1).astype(int)
    main_top["—Å—Ä–µ–¥–Ω–∏–π_–≤–æ–∑—Ä–∞—Å—Ç_–¥–µ—Ç–µ–π"] = main_top["—Å—Ä–µ–¥–Ω–∏–π_–≤–æ–∑—Ä–∞—Å—Ç_–¥–µ—Ç–µ–π"].fillna(0).astype(float)
    main_top["—Å—Ä_–∑–ø"] = main_top["—Å—Ä_–∑–ø"].fillna(0).astype(float)

    for col in FLOAT_COLS:
        if col in main_top.columns:
            main_top[col] = pd.to_numeric(main_top[col], errors="coerce")
            main_top[col] = main_top[col].fillna(main_top[col].median())

    main_top.to_csv(f"{DATA_PROCESSED}/main_top.csv", index=False)

    # üëâ –ù–æ–≤—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç–æ–ª—å–∫–æ –¥–ª—è main_top
    preprocessor_top = DataPreprocessor()
    main_top_for_train = preprocessor_top.fit(main_top)

    main_top_for_train.to_csv(f"{DATA_PROCESSED}/main_top_for_train.csv", index=False)

    # üíæ –°–æ—Ö—Ä–∞–Ω–∏–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    preprocessor_top.save(f"{DATA_PROCESSED}/preprocessor_top")

    print(
        "NaNs in main_top_for_train:\n",
        main_top_for_train.isnull().sum()[main_top_for_train.isnull().any()],
    )


def run_all():
    main_prepare_for_all(main_users, users_salary, users_cadr, children)
    prepare_with_mic()


if __name__ == "__main__":
    logger.info("–§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–∑ –Ω–∞—á–∞–ª–∞—Å—å")
    run_all()
    logger.info("–§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–∑ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å")
