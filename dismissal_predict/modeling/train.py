from collections import Counter
from datetime import datetime
import logging
import os
import warnings

import joblib
import matplotlib.pyplot as plt
import mlflow
import mlflow.sklearn
import numpy as np
import optuna
import pandas as pd
import shap  # type: ignore
from sklearn.exceptions import UndefinedMetricWarning
from sklearn.metrics import (
    ConfusionMatrixDisplay,
    RocCurveDisplay,
    accuracy_score,
    confusion_matrix,
    f1_score,
    precision_recall_fscore_support,
    precision_score,
    recall_score,
    roc_auc_score,
)
from sklearn.model_selection import StratifiedKFold, train_test_split
from tqdm import tqdm
from xgboost import XGBClassifier

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏ –ª—É—á—à–µ –∏–ª–∏ —Ö—É–∂–µ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å
GREEN = "\033[92m"
RED = "\033[91m"
RESET = "\033[0m"

DATA_PROCESSED = "/home/root6/python/dismissal_predict_v2/data/processed"
MODELS = "/home/root6/python/dismissal_predict_v2/models"
os.makedirs(MODELS, exist_ok=True)

INPUT_FILE_MAIN_USERS = f"{DATA_PROCESSED}/main_users_for_train.csv"
INPUT_FILE_TOP_USERS = f"{DATA_PROCESSED}/main_top_for_train.csv"

TEST_SIZE = 0.2
RANDOM_STATE = 40
N_TRIALS = 10  # –∏—Ç—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–ø—Ç—É–Ω—ã
N_SPLITS = 15  # —á–∏—Å–ª–æ –∫—Ä–æ—Å—Å–≤–∞–ª–∏–¥–∞—Ü–∏–π
METRIC = "custom"
EVAL_METRIC = "logloss"
MLFLOW_EXPERIMENT_MAIN = "xgboost_main_users"
MLFLOW_EXPERIMENT_TOP = "xgboost_top_users"

TARGET_COL = "—É–≤–æ–ª–µ–Ω"


warnings.filterwarnings("ignore")

main_users = pd.read_csv(INPUT_FILE_MAIN_USERS, delimiter=",", decimal=",")
top_users = pd.read_csv(INPUT_FILE_TOP_USERS, delimiter=",", decimal=",")


def custom_metric(recall, precision):
    metric = 0.8 * recall + 0.2 * precision
    return metric


def is_new_model_better(new_metrics, old_metrics, delta=0.001):
    def round3(x):
        return round(x or 0, 3)

    new_f1 = round3(new_metrics.get("f1"))
    old_f1 = round3(old_metrics.get("f1"))

    if new_f1 > old_f1:
        return True
    if new_f1 < old_f1:
        return False

    # f1 —Ä–∞–≤–Ω—ã ‚Äî —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º recall
    new_recall = round3(new_metrics.get("recall"))
    old_recall = round3(old_metrics.get("recall"))

    if new_recall > old_recall:
        return True
    if new_recall < old_recall:
        return False

    # recall —Ä–∞–≤–Ω—ã ‚Äî —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º precision
    new_precision = round3(new_metrics.get("precision"))
    old_precision = round3(old_metrics.get("precision"))

    return new_precision > old_precision


def manual_optuna_progress(study, n_trials, func):
    for _ in tqdm(range(n_trials), desc="Optuna Tuning"):
        study.optimize(func, n_trials=1, catch=(Exception,), n_jobs=4)


def convert_all_to_float(df: pd.DataFrame, exclude_cols=None):
    df = df.copy()
    exclude_cols = exclude_cols or []

    for col in df.columns:
        if col in exclude_cols:
            continue

        df[col] = (
            df[col]
            .astype(str)
            .str.replace(",", ".", regex=False)
            .str.replace(" ", "", regex=False)
            .str.strip()
            .replace(["nan", "None", "", "NaN"], np.nan)
        )
        df[col] = pd.to_numeric(df[col], errors="coerce")

    return df


def cross_val_best_threshold(
    model,
    X,
    y,
    thresholds=np.arange(0.01, 0.9, 0.01),
    metric=METRIC,
    n_splits=N_SPLITS,
    random_state=RANDOM_STATE,
):
    print(f"–ü–æ–∏—Å–∫ threshold –ø–æ –º–µ—Ç—Ä–∏–∫–µ {metric.upper()} —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ FN + FP...\n")

    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    fold_results = []

    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        model.fit(X_train, y_train)
        y_probs = model.predict_proba(X_val)[:, 1]

        best_t = None
        best_metric = -1
        best_fn_fp = float("inf")
        best_fn = best_fp = None

        for t in thresholds:
            y_pred = (y_probs >= t).astype(int)

            if metric == "f1":
                score = f1_score(y_val, y_pred, zero_division=0)
            elif metric == "precision":
                score = precision_score(y_val, y_pred, zero_division=0)
            elif metric == "recall":
                score = recall_score(y_val, y_pred, zero_division=0)
            elif metric == "roc_auc":
                score = roc_auc_score(y_val, y_probs)
            elif metric == "custom":
                recall = recall_score(y_val, y_pred, zero_division=0)
                precision = precision_score(y_val, y_pred, zero_division=0)
                score = custom_metric(recall, precision)
            else:
                raise ValueError(f"Unsupported metric: {metric}")

            cm = confusion_matrix(y_val, y_pred, labels=[0, 1])
            tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (0, 0, 0, 0)
            fn_fp = fn + fp

            if score > best_metric or (score == best_metric and fn_fp < best_fn_fp):
                best_metric = score
                best_t = t
                best_fn_fp = fn_fp
                best_fn = fn
                best_fp = fp

        fold_results.append(
            {
                "fold": fold,
                "threshold": best_t,
                "metric": best_metric,
                "fn": best_fn,
                "fp": best_fp,
                "fn_fp": best_fn_fp,
            }
        )

        print(
            f"[Fold {fold}] ‚úÖ threshold = {best_t:.3f} ‚Üí {metric} = {best_metric:.4f}, FN+FP = {best_fn_fp}"
        )

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –º–µ—Ç—Ä–∏–∫–µ –ø–æ —É–±—ã–≤–∞–Ω–∏—é, –¥–∞–ª–µ–µ –ø–æ fn+fp –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é
    fold_results = sorted(fold_results, key=lambda x: (-x["metric"], x["fn_fp"]))

    top_k = n_splits // 2  # –ø–æ–ª–æ–≤–∏–Ω–∞ —Ñ–æ–ª–¥–æ–≤
    top_folds = fold_results[:top_k]

    mean_t = np.mean([r["threshold"] for r in top_folds])
    mean_metric = np.mean([r["metric"] for r in top_folds])
    mean_fn = np.mean([r["fn"] for r in top_folds])
    mean_fp = np.mean([r["fp"] for r in top_folds])

    print(
        f"\nüéØ –§–∏–Ω–∞–ª—å–Ω—ã–π threshold = {mean_t:.3f} ‚Üí {metric} = {mean_metric:.4f}, —Å—Ä–µ–¥–Ω–∏–µ FN = {mean_fn:.1f}, FP = {mean_fp:.1f} (–ø–æ top-{top_k} —Ñ–æ–ª–¥–∞–º)"
    )
    return mean_t, mean_metric, mean_fn, mean_fp


def custom_cv_score(model, X, y, threshold, n_splits=N_SPLITS, metric=METRIC):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    scores = []

    for train_idx, valid_idx in skf.split(X, y):
        X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]
        y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]

        model.fit(X_train_fold, y_train_fold)
        y_probs = model.predict_proba(X_valid_fold)[:, 1]
        y_preds = (y_probs >= threshold).astype(int)

        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", UndefinedMetricWarning)

                if metric == "f1":
                    score = f1_score(y_valid_fold, y_preds)
                elif metric == "recall":
                    score = recall_score(y_valid_fold, y_preds)
                elif metric == "precision":
                    score = precision_score(y_valid_fold, y_preds)
                elif metric == "roc_auc":
                    score = roc_auc_score(y_valid_fold, y_probs)
                else:
                    raise ValueError(f"Unsupported metric: {metric}")

            if not np.isnan(score):
                scores.append(score)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –Ω–∞ —Ñ–æ–ª–¥–µ: {e}")
            continue

    return np.mean(scores) if scores else float("nan")


def split_df(data):
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            data.drop([TARGET_COL], axis=1),
            data[TARGET_COL],
            test_size=TEST_SIZE,
            random_state=RANDOM_STATE,
            stratify=data[TARGET_COL],
        )
        return X_train, X_test, y_train, y_test
    except Exception as e:
        logger.info(f"–û—à–∏–±–∫–∞: {e}")
        raise


def objective(trial, X_train, y_train, threshold):
    try:
        counter = Counter(y_train)
        scale_pos_weight = counter[0] / counter[1]

        params = {
            "n_estimators": trial.suggest_int("n_estimators", 10, 500),
            "max_depth": trial.suggest_int("max_depth", 2, 300),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.7),
            "subsample": trial.suggest_float("subsample", 0.2, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.2, 1.0),
            "scale_pos_weight": scale_pos_weight,
            "random_state": RANDOM_STATE,
            "eval_metric": EVAL_METRIC,
        }

        skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)

        recalls, precisions, f1s, accuracies = [], [], [], []
        fn_list, fp_list, tn_list, tp_list = [], [], [], []

        for train_idx, valid_idx in skf.split(X_train, y_train):
            model = XGBClassifier(**params)
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]

            model.fit(X_tr, y_tr)
            y_proba = model.predict_proba(X_val)[:, 1]
            y_pred = (y_proba >= threshold).astype(int)

            recalls.append(recall_score(y_val, y_pred))
            precisions.append(precision_score(y_val, y_pred, zero_division=0))
            f1s.append(f1_score(y_val, y_pred, zero_division=0))
            accuracies.append(accuracy_score(y_val, y_pred))

            cm = confusion_matrix(y_val, y_pred, labels=[0, 1])
            if cm.shape == (2, 2):
                tn, fp, fn, tp = cm.ravel()
            else:
                tn = cm[0, 0] if cm.shape[0] > 0 and cm.shape[1] > 0 else 0
                fp = cm[0, 1] if cm.shape[0] > 0 and cm.shape[1] > 1 else 0
                fn = cm[1, 0] if cm.shape[0] > 1 and cm.shape[1] > 0 else 0
                tp = cm[1, 1] if cm.shape[0] > 1 and cm.shape[1] > 1 else 0

            fn_list.append(fn)
            fp_list.append(fp)
            tn_list.append(tn)
            tp_list.append(tp)

        # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —Ñ–æ–ª–¥–∞–º
        mean_recall = np.mean(recalls)
        mean_precision = np.mean(precisions)
        mean_f1 = np.mean(f1s)
        mean_accuracy = np.mean(accuracies)

        # –í—ã–±–æ—Ä –º–µ—Ç—Ä–∏–∫–∏
        if METRIC == "f1":
            score = mean_f1
        elif METRIC == "accuracy":
            score = mean_accuracy
        elif METRIC == "recall":
            score = mean_recall
        elif METRIC == "precision":
            score = mean_precision
        elif METRIC == "custom":
            score = custom_metric(mean_recall, mean_precision)
        else:
            logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ '{METRIC}', –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è recall –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
            score = mean_recall

        logger.info(
            f"Trial {trial.number} ‚Üí "
            f"Recall: {mean_recall:.3f}, Precision: {mean_precision:.3f}, F1: {mean_f1:.3f}, "
            f"Accuracy: {mean_accuracy:.3f}, Score: {score:.3f} | "
            f"FN: {np.mean(fn_list):.1f}, FP: {np.mean(fp_list):.1f}, TN: {np.mean(tn_list):.1f}, TP: {np.mean(tp_list):.1f}"
        )

        return score

    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –≤ objective: {e}")
        return -1


def run_optuna_experiment(
    X_train,
    y_train,
    X_test,
    y_test,
    metric,
    n_trials,
    experiment_name,
    model_output_path,
    current_time,
    global_threshold,
):
    try:
        counter = Counter(y_train)
        scale_pos_weight = counter[0] / counter[1]
        logger.info(f"Tracking URI: {mlflow.get_tracking_uri()}")
        logger.info(f"Experiment: {experiment_name}")
        mlflow.set_experiment(experiment_name)

        def optuna_objective(trial):
            return objective(trial, X_train, y_train, global_threshold)

        study = optuna.create_study(study_name=experiment_name, direction="maximize")
        manual_optuna_progress(study, n_trials, optuna_objective)

        best_params = study.best_trial.params

        final_model = XGBClassifier(
            **best_params,
            scale_pos_weight=scale_pos_weight,
            random_state=RANDOM_STATE,
            eval_metric=EVAL_METRIC,
        )
        final_model.fit(X_train, y_train)
        y_pred_proba = final_model.predict_proba(X_test)[:, 1]
        y_pred = (y_pred_proba >= global_threshold).astype(int)
        recall = recall_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)

        final_metrics = {
            "accuracy": accuracy_score(y_test, y_pred),
            "precision": precision,
            "recall": recall,
            "roc_auc": roc_auc_score(y_test, y_pred_proba),
            "f1": f1_score(y_test, y_pred),
            "custom": custom_metric(recall, precision),
        }

        save_model = True
        if os.path.exists(model_output_path):
            try:
                old_model_bundle = joblib.load(model_output_path)
                old_metrics = old_model_bundle.get("metrics", {})
                logger.info(f"–°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å: {old_metrics}")
                logger.info(f"–ù–æ–≤–∞—è –º–æ–¥–µ–ª—å: {final_metrics}")
                save_model = is_new_model_better(final_metrics, old_metrics)

                if save_model:
                    logger.info(f"{GREEN}–ù–æ–≤–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º.{RESET}")
                else:
                    logger.info(f"{RED}–°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ ‚Äî –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—É—é.{RESET}")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ä—É—é –º–æ–¥–µ–ª—å: {e}. –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—É—é.")
                save_model = True

        if save_model:
            joblib.dump(
                {
                    "model": final_model,
                    "threshold": global_threshold,
                    "metrics": final_metrics,
                },
                model_output_path,
            )
            logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_output_path}")

        input_example = pd.DataFrame(X_test[:1], columns=X_test.columns)
        run_name = f"model_{current_time}"

        # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç—Ä–µ–π–Ω–µ
        y_pred_proba_train = final_model.predict_proba(X_train)[:, 1]
        y_pred_train = (y_pred_proba_train >= global_threshold).astype(int)
        recall_train = recall_score(y_train, y_pred_train)
        precision_train = precision_score(y_train, y_pred_train)

        final_metrics_train = {
            "accuracy": accuracy_score(y_train, y_pred_train),
            "precision": precision_train,
            "recall": recall_train,
            "roc_auc": roc_auc_score(y_train, y_pred_proba_train),
            "f1": f1_score(y_train, y_pred_train),
            "custom": custom_metric(recall_train, precision_train),
        }

        if mlflow.active_run():
            mlflow.end_run()

        with mlflow.start_run(run_name=run_name):
            mlflow.log_params(best_params)
            mlflow.log_param("test_size", TEST_SIZE)
            mlflow.log_param("random_state", RANDOM_STATE)
            mlflow.log_param("n_trials", n_trials)
            mlflow.log_param("model_type", "XGBoostClassifier")
            mlflow.log_param("threshold", round(global_threshold, 4))
            mlflow.log_param("global_threshold", round(global_threshold, 4))

            mlflow.log_param("opt_metric", f"{METRIC}")

            mlflow.log_metric("f1_train", round(final_metrics_train["f1"], 3))
            mlflow.log_metric("f1_test", round(final_metrics["f1"], 3))

            mlflow.log_metric("accuracy_train", round(final_metrics_train["accuracy"], 3))
            mlflow.log_metric("accuracy_test", round(final_metrics["accuracy"], 3))

            mlflow.log_metric("recall_train", round(final_metrics_train["recall"], 3))
            mlflow.log_metric("recall_test", round(final_metrics["recall"], 3))

            mlflow.log_metric("precision_train", round(final_metrics_train["precision"], 3))
            mlflow.log_metric("precision_test", round(final_metrics["precision"], 3))

            mlflow.log_metric("roc_auc_train", round(final_metrics_train["roc_auc"], 3))
            mlflow.log_metric("roc_auc_test", round(final_metrics["roc_auc"], 3))

            mlflow.log_metric("custom_train", round(final_metrics_train["custom"], 3))
            mlflow.log_metric("custom_test", round(final_metrics["custom"], 3))

            mlflow.log_artifact(model_output_path)
            mlflow.sklearn.log_model(final_model, name="final_model", input_example=input_example)  # type: ignore

            # Confusion matrix
            fig_cm, ax_cm = plt.subplots()
            ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax_cm)
            fig_cm.tight_layout()
            fig_cm.savefig("confusion_matrix.png")
            plt.close(fig_cm)
            mlflow.log_artifact("confusion_matrix.png")
            os.remove("confusion_matrix.png")

            # ROC curve
            fig_roc, ax_roc = plt.subplots()
            RocCurveDisplay.from_predictions(y_test, y_pred_proba, ax=ax_roc)
            fig_roc.tight_layout()
            fig_roc.savefig("roc_curve.png")
            plt.close(fig_roc)
            mlflow.log_artifact("roc_curve.png")
            os.remove("roc_curve.png")

            # SHAP summary
            explainer = shap.Explainer(final_model)
            shap_values = explainer(X_test)
            plt.figure()  # –≤–∞–∂–Ω–æ!
            shap.summary_plot(shap_values, X_test, plot_type="dot", show=False)
            plt.tight_layout()
            plt.savefig("shap_dot_plot.png")
            plt.close()
            mlflow.log_artifact("shap_dot_plot.png")
            os.remove("shap_dot_plot.png")

            # Threshold vs metrics
            thresholds = np.arange(0.1, 0.9, 0.01)
            f1s, precisions, recalls = [], [], []
            for t in thresholds:
                y_pred_temp = (y_pred_proba >= t).astype(int)
                p, r, f1, _ = precision_recall_fscore_support(
                    y_test, y_pred_temp, average="binary", zero_division=0
                )
                f1s.append(f1)
                precisions.append(p)
                recalls.append(r)

            plt.figure(figsize=(8, 6))
            plt.plot(thresholds, f1s, label="F1")
            plt.plot(thresholds, precisions, label="Precision")
            plt.plot(thresholds, recalls, label="Recall")
            plt.axvline(
                global_threshold,
                color="gray",
                linestyle="--",
                label=f"Threshold = {round(global_threshold, 3)}",
            )
            plt.xlabel("Threshold")
            plt.ylabel("Score")
            plt.title("Threshold vs F1 / Precision / Recall")
            plt.legend()
            plt.grid(True)
            plt.tight_layout()
            plt.savefig("threshold_metrics.png")
            plt.close()
            mlflow.log_artifact("threshold_metrics.png")
            os.remove("threshold_metrics.png")

            # Histogram of predicted probabilities
            plt.figure(figsize=(8, 6))
            plt.hist(y_pred_proba, bins=50, alpha=0.7)
            plt.axvline(
                global_threshold,
                color="red",
                linestyle="--",
                label=f"Threshold = {round(global_threshold, 3)}",
            )
            plt.title("Distribution of predicted probabilities")
            plt.xlabel("Probability")
            plt.ylabel("Count")
            plt.legend()
            plt.grid(True)
            plt.tight_layout()
            plt.savefig("proba_distribution.png")
            plt.close()
            mlflow.log_artifact("proba_distribution.png")
            os.remove("proba_distribution.png")

    except Exception as e:
        logger.info(f"–û—à–∏–±–∫–∞: {e}")
        raise


def today():
    now = pd.to_datetime(datetime.now())
    return now


if __name__ == "__main__":
    mlflow.set_tracking_uri("file:///home/root6/python/dismissal_predict_v2/mlruns")

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
    main_users = convert_all_to_float(main_users, exclude_cols=[TARGET_COL])
    top_users = convert_all_to_float(top_users, exclude_cols=[TARGET_COL])
    main_users[TARGET_COL] = (
        pd.to_numeric(main_users[TARGET_COL], errors="coerce").fillna(0).astype(int)
    )
    top_users[TARGET_COL] = (
        pd.to_numeric(top_users[TARGET_COL], errors="coerce").fillna(0).astype(int)
    )

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    main_users = main_users.drop_duplicates()
    top_users = top_users.drop_duplicates()

    # –°–ø–ª–∏—Ç–∏–º –¥–∞–Ω–Ω—ã–µ –æ–¥–∏–Ω —Ä–∞–∑!
    X_main = main_users.drop(columns=[TARGET_COL])
    y_main = main_users[TARGET_COL]
    X_top = top_users.drop(columns=[TARGET_COL])
    y_top = top_users[TARGET_COL]

    # --- MAIN USERS ---
    base_model_main = XGBClassifier(
        scale_pos_weight=Counter(y_main)[0] / Counter(y_main)[1],
        random_state=RANDOM_STATE,
        eval_metric=EVAL_METRIC,
    )

    global_threshold_main, f1, fn, fp = cross_val_best_threshold(
        base_model_main, X_main, y_main, metric=METRIC
    )

    print(
        f"–í—ã–±—Ä–∞–Ω–Ω—ã–π threshold –¥–ª—è all = {global_threshold_main:.3f}, f1 = {f1:.4f}, fn = {fn:.4f}, fp = {fp:.4f}"
    )

    X_train_main, X_test_main, y_train_main, y_test_main = train_test_split(
        X_main, y_main, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_main
    )

    run_optuna_experiment(
        X_train=X_train_main,
        y_train=y_train_main,
        X_test=X_test_main,
        y_test=y_test_main,
        metric=METRIC,
        n_trials=N_TRIALS,
        experiment_name=MLFLOW_EXPERIMENT_MAIN,
        model_output_path=f"{MODELS}/xgb_main_users.pkl",
        current_time=today(),
        global_threshold=global_threshold_main,
    )

    # --- TOP USERS ---
    base_model_top = XGBClassifier(
        scale_pos_weight=Counter(y_top)[0] / Counter(y_top)[1],
        random_state=RANDOM_STATE,
        eval_metric=EVAL_METRIC,
    )

    global_threshold_top, f1, fn, fp = cross_val_best_threshold(
        base_model_top, X_top, y_top, metric=METRIC
    )

    print(
        f"–í—ã–±—Ä–∞–Ω–Ω—ã–π threshold –¥–ª—è top = {global_threshold_main:.3f}, f1 = {f1:.4f}, fn = {fn:.4f}, fp = {fp:.4f}"
    )

    X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(
        X_top, y_top, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_top
    )

    run_optuna_experiment(
        X_train=X_train_top,
        y_train=y_train_top,
        X_test=X_test_top,
        y_test=y_test_top,
        metric=METRIC,
        n_trials=N_TRIALS,
        experiment_name=MLFLOW_EXPERIMENT_TOP,
        model_output_path=f"{MODELS}/xgb_top_users.pkl",
        current_time=today(),
        global_threshold=global_threshold_top,
    )
