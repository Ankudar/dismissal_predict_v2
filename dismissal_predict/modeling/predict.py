from datetime import datetime
import logging
import os
import sys

import joblib
import pandas as pd
import shap

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from dismissal_predict import DROP_COLS, FLOAT_COLS, DataPreprocessor

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# –ü—É—Ç–∏
DATA_PROCESSED = "/home/root6/python/dismissal_predict_v2/data/processed"
DATA_INTERIM = "/home/root6/python/dismissal_predict_v2/data/interim"
RESULTS_DIR = "/home/root6/python/dismissal_predict_v2/data/results"
MODELS_DIR = "/home/root6/python/dismissal_predict_v2/models"
os.makedirs(RESULTS_DIR, exist_ok=True)

# –§–∞–π–ª—ã
INPUT_FILE_MAIN_ALL = os.path.join(DATA_PROCESSED, "main_all.csv")
INPUT_FILE_MAIN_TOP = os.path.join(DATA_PROCESSED, "main_top.csv")
INPUT_FILE_CADR = os.path.join(DATA_INTERIM, "check_last_users_update.csv")
MODEL_MAIN = os.path.join(MODELS_DIR, "main_users.pkl")
MODEL_TOP = os.path.join(MODELS_DIR, "top_users.pkl")
PREPROCESSOR_MAIN_PATH = os.path.join(DATA_PROCESSED, "preprocessor.pkl")
PREPROCESSOR_TOP_PATH = os.path.join(DATA_PROCESSED, "preprocessor_top.pkl")

# –î–∞–Ω–Ω—ã–µ
df_cadr = pd.read_csv(INPUT_FILE_CADR, delimiter=",", decimal=",")
df_main_all = pd.read_csv(INPUT_FILE_MAIN_ALL, delimiter=",", decimal=",")
df_main_top = pd.read_csv(INPUT_FILE_MAIN_TOP, delimiter=",", decimal=",")


def load_model_and_threshold(model_path):
    model_bundle = joblib.load(model_path)
    return (
        model_bundle["model"],
        model_bundle["threshold"],
        model_bundle.get("selected_features", None),
    )


def update_results_with_cadr(result_df, main_df):
    result_df["—É–≤–æ–ª–µ–Ω"] = 0  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º –≤—Å–µ—Ö –Ω–µ —É–≤–æ–ª–µ–Ω–Ω—ã–º–∏

    # —à–∞–≥ 1 ‚Äî —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ main_df
    for fio in result_df["—Ñ–∏–æ"]:
        if fio in main_df["—Ñ–∏–æ"].values:
            is_fired = main_df.loc[main_df["—Ñ–∏–æ"] == fio, "—É–≤–æ–ª–µ–Ω"].values[0]
            result_df.loc[result_df["—Ñ–∏–æ"] == fio, "—É–≤–æ–ª–µ–Ω"] = int(float(is_fired))

    # —à–∞–≥ 2 ‚Äî –µ—Å–ª–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ –Ω–µ—Ç –≤ –∫–∞–¥—Ä–æ–≤–æ–º df_cadr (–§–ò–û –≤ –≤–∏–¥–µ "–§–∞–º–∏–ª–∏—è –ò–º—è –û—Ç—á–µ—Å—Ç–≤–æ")
    cadr_fio_short = (
        df_cadr["—Ñ–∏–æ"].dropna().apply(lambda x: " ".join(x.strip().split()[:2])).unique()
    )
    for fio in result_df["—Ñ–∏–æ"]:
        if fio not in cadr_fio_short:
            result_df.loc[result_df["—Ñ–∏–æ"] == fio, "—É–≤–æ–ª–µ–Ω"] = 1

    return result_df


def add_predictions_to_excel(original_df, model, threshold, result_file, preprocessor, features):
    try:
        original_df["—É–≤–æ–ª–µ–Ω"] = original_df["–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"].notna().astype(int)

        if "—Ñ–∏–æ" not in original_df.columns:
            raise ValueError("–í –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ '—Ñ–∏–æ'")

        display_df = original_df[["—Ñ–∏–æ", "—É–≤–æ–ª–µ–Ω"]].copy()
        predict_df = original_df[original_df["–¥–∞—Ç–∞_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"].isna()].copy()

        predict_df_clean = predict_df.drop(
            columns=[col for col in DROP_COLS if col in predict_df.columns]
        )
        predict_clean = preprocessor.transform(predict_df_clean)
        predict_clean = predict_clean.apply(pd.to_numeric, errors="coerce").fillna(0)
        predict_clean = predict_clean.drop(columns=["—É–≤–æ–ª–µ–Ω"], errors="ignore")

        if features:
            predict_clean = predict_clean[features]

        probabilities = model.predict_proba(predict_clean)[:, 1]
        predictions = (probabilities >= threshold).astype(int)
        today = datetime.today().strftime("%d.%m.%Y")

        result_today = pd.DataFrame(
            {
                "—Ñ–∏–æ": predict_df["—Ñ–∏–æ"].values,
                "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è": predictions,
                today: probabilities,
            }
        )

        # üîπ SHAP: —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
        explainer = shap.Explainer(model)
        shap_values = explainer(predict_clean)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if not features:
            features = list(predict_clean.columns)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ SHAP –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –º–æ–¥–µ–ª–∏ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
        if shap_values.values.ndim == 3:
            # –ï—Å–ª–∏ 3D ‚Äî –±–µ—Ä—ë–º SHAP –¥–ª—è –∫–ª–∞—Å—Å–∞ 1
            shap_vals_for_class1 = shap_values.values[:, :, 1]
        elif shap_values.values.ndim == 2:
            shap_vals_for_class1 = shap_values.values
        else:
            raise ValueError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç SHAP-–∑–Ω–∞—á–µ–Ω–∏–π: ndim={shap_values.values.ndim}")

        shap_df = pd.DataFrame(shap_vals_for_class1, columns=features)
        shap_df.insert(0, "—Ñ–∏–æ", predict_df["—Ñ–∏–æ"].values)

        shap_path = result_file.replace(".xlsx", "_shap.csv")
        shap_df.to_csv(shap_path, index=False, encoding="utf-8-sig")
        logger.info(f"SHAP –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {shap_path}")

        # üîπ –û–±–Ω–æ–≤–ª—è–µ–º Excel
        if os.path.exists(result_file):
            final_df = pd.read_excel(result_file)
            final_df = update_results_with_cadr(final_df, original_df)
        else:
            final_df = display_df.copy()
            final_df["–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"] = None
            final_df = update_results_with_cadr(final_df, original_df)

        for _, row in result_today.iterrows():
            fio = row["—Ñ–∏–æ"]
            if fio in final_df["—Ñ–∏–æ"].values:
                final_df.loc[final_df["—Ñ–∏–æ"] == fio, today] = row[today]
                final_df.loc[final_df["—Ñ–∏–æ"] == fio, "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"] = row[
                    "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"
                ]
            else:
                new_row = {
                    "—Ñ–∏–æ": fio,
                    "—É–≤–æ–ª–µ–Ω": 0,
                    "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è": row["–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ_—É–≤–æ–ª—å–Ω–µ–Ω–∏—è"],
                    today: row[today],
                }
                final_df = pd.concat([final_df, pd.DataFrame([new_row])], ignore_index=True)

        final_df.to_excel(result_file, index=False)
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ {result_file}")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ Excel: {e}")
        raise


if __name__ == "__main__":
    logger.info("–ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")

    try:
        # üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
        preprocessor_main = DataPreprocessor.load(PREPROCESSOR_MAIN_PATH)
        preprocessor_top = DataPreprocessor.load(PREPROCESSOR_TOP_PATH)

        model_main, threshold_main, features_main = load_model_and_threshold(MODEL_MAIN)
        model_top, threshold_top, features_top = load_model_and_threshold(MODEL_TOP)

        if not features_main:
            features_main = [
                col for col in df_main_all.columns if col != "—É–≤–æ–ª–µ–Ω" and col not in DROP_COLS
            ]

        if not features_top:
            features_top = [
                col for col in df_main_top.columns if col != "—É–≤–æ–ª–µ–Ω" and col not in DROP_COLS
            ]

        # –ß–∏—Å—Ç–∏–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        df_main_all_clean = df_main_all.drop(
            columns=[col for col in DROP_COLS if col in df_main_all.columns]
        )
        df_main_top_clean = df_main_top.drop(
            columns=[col for col in DROP_COLS if col in df_main_top.columns]
        )

        for col in FLOAT_COLS:
            if col in df_main_all_clean.columns:
                df_main_all_clean[col] = pd.to_numeric(df_main_all_clean[col], errors="coerce")
                df_main_all_clean[col] = df_main_all_clean[col].fillna(
                    df_main_all_clean[col].median()
                )
            if col in df_main_top_clean.columns:
                df_main_top_clean[col] = pd.to_numeric(df_main_top_clean[col], errors="coerce")
                df_main_top_clean[col] = df_main_top_clean[col].fillna(
                    df_main_top_clean[col].median()
                )

        # üîπ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞–º–∏
        df_main_all_proc = preprocessor_main.transform(df_main_all_clean)
        df_main_top_proc = preprocessor_top.transform(df_main_top_clean)

        features_main = [f for f in features_main if f in df_main_all_proc.columns]
        features_top = [f for f in features_top if f in df_main_top_proc.columns]

        df_main_all_proc = (
            df_main_all_proc[features_main + ["—É–≤–æ–ª–µ–Ω"]]
            if "—É–≤–æ–ª–µ–Ω" in df_main_all_proc.columns
            else df_main_all_proc[features_main]
        )
        df_main_top_proc = (
            df_main_top_proc[features_top + ["—É–≤–æ–ª–µ–Ω"]]
            if "—É–≤–æ–ª–µ–Ω" in df_main_top_proc.columns
            else df_main_top_proc[features_top]
        )

        # üîπ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        add_predictions_to_excel(
            df_main_all.copy(),
            model_main,
            threshold_main,
            os.path.join(RESULTS_DIR, "result_all.xlsx"),
            preprocessor_main,
            features_main,
        )

        add_predictions_to_excel(
            df_main_top.copy(),
            model_top,
            threshold_top,
            os.path.join(RESULTS_DIR, "result_top.xlsx"),
            preprocessor_top,
            features_top,
        )

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Å–∫—Ä–∏–ø—Ç–∞: {e}")
